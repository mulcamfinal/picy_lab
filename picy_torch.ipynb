{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"picy_torch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMDzbhC3syi3zCv0dmLfc4A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ikLWvQYXA3d2"},"source":["# Endocder, Attention and Decoder\n","# Show, Attend and Tell\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torchvision.models as tvmodels\n","\n","\n","class Encoder(nn.Module):\n","    \"\"\"\n","    An encoder that encodes each input image to tensor with shape (L, D)\n","    resnet101 is used for pretrained convolutional network.\n","    \"\"\"\n","    def __init__(self, encoded_size=14, encoder_finetune=False):\n","        \"\"\"\n","        :param encoded_size: size of image after being encoded.\n","        :param allow_finetune: if allow finetune, then encoder conv network is also trained.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.encoded_size = encoded_size\n","\n","        resnet101 = tvmodels.resnet101(pretrained=True)\n","        layers_to_use = list(resnet101.children())[:-3]\n","\n","        self.conv_net = nn.Sequential(*layers_to_use)\n","\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_size, encoded_size))\n","\n","        self.encoder_finetune = encoder_finetune\n","        \n","        if not encoder_finetune:\n","            self.conv_net.eval()\n","\n","    def forward(self, images):\n","        \"\"\"\n","        :param images: Tensor with shape (batch_size, 3, image_size, image_size)\n","        :return:\n","        \"\"\"\n","        if not self.encoder_finetune:\n","            with torch.no_grad():\n","                x = self.conv_net(images)\n","        else:\n","            x = self.conv_net(images)  # (batch_size, encoder_dim, image_size/32, image_size/32\n","        x = self.adaptive_pool(x)  # (batch_size, encoder_dim, self.encoded_size, self.encoded_size\n","        x = x.permute(0, 2, 3, 1)  # (batch_size, self.encoded_size, self.encoded_size, encoder_dim\n","        # since shape (batch_size, self.encoded_size ** 2, encoder_dim) will be used in decoder, do permutation\n","\n","        batch_size = x.shape[0]\n","        encoder_dim = x.shape[-1]\n","        x = x.view(batch_size, -1, encoder_dim)  # (batch_size, L, D)\n","        # each point l in encoded image has vector with D-dim that represents that point\n","        # self.encoded_size ** 2 will be L and encoder_dim will be D in original paper's notation\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    \"\"\"\n","    Deterministic \"soft\" attention, which is differentiable and thus can be learned by backpropagation\n","    \"\"\"\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        \"\"\"\n","        :param encoder_dim: feature size of encoded image = last dimension of encoder output.\n","        :param decoder_dim: dimension of decoder's hidden state\n","        :param attention_dim: size of attention network. does not affect output dimension\n","        \"\"\"\n","        super().__init__()\n","        self.encoder_attention = nn.Linear(encoder_dim, attention_dim)\n","        self.decoder_attention = nn.Linear(decoder_dim, attention_dim)\n","        self.attention = nn.Linear(attention_dim, 1)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, encoder_output, decoder_hidden):\n","        encoder_att = self.encoder_attention(encoder_output)  # (batch_size, L, attention_dim)\n","        decoder_att = self.decoder_attention(decoder_hidden)  # (batch_size, attention_dim)\n","        encoder_plus_decoder_att = encoder_att + decoder_att.unsqueeze(1)  # (batch_size, L, attention_dim)\n","        attention = self.attention(F.relu(encoder_plus_decoder_att)).squeeze(2)  # (batch_size, L)\n","        alpha = self.softmax(attention)  # (batch_size, L)\n","        context_vector = (encoder_output * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n","        # sum(dim=1) means summing over L\n","        # context vector is z_hat in original paper, calculated from h_t-1, and encoder output a\n","\n","        return context_vector, alpha  # keep alpha for visualization?\n","\n","\n","class Decoder(nn.Module):\n","    \"\"\"\n","    Decoder with attention\n","    \"\"\"\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim, embed_dim, vocab_size,\n","                 dropout=0.5, embedding_finetune=True):\n","        \"\"\"\n","        :param encoder_dim: feature size of encoded image = last dimension of encoder output.\n","        :param decoder_dim: dimension of decoder's hidden state\n","        :param attention_dim: size of attention network. does not affect output dimension\n","        :param embed_dim: dimension of word embedding\n","        :param vocab_size: size of vocabulary\n","        :param dropout: dropout rate\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        self.encoder_dim = encoder_dim\n","        self.decoder_dim = decoder_dim\n","        self.attention_dim = attention_dim\n","        self.embed_dim = embed_dim\n","        self.vocab_size = vocab_size\n","        self.dropout_rate = dropout\n","\n","        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","\n","        self.lstm = nn.LSTMCell(self.encoder_dim + self.embed_dim, self.decoder_dim, bias=True)\n","        # h and c are initialized from encoder output.\n","        # authors used MLP, for now, use single layer perceptron\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim, bias=False)\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim, bias=False)\n","\n","        # deep output layers\n","        self.L_h = nn.Linear(decoder_dim, embed_dim, bias=False)\n","        self.L_z = nn.Linear(encoder_dim, embed_dim, bias=False)\n","        self.L_o = nn.Linear(embed_dim, vocab_size, bias=False)\n","\n","        self.dropout = nn.Dropout(p=self.dropout_rate)\n","\n","        if not embedding_finetune:\n","            # always set embedding_finetune == True when not using pretrained embeddings\n","            for param in self.embedding.parameters():\n","                param.requires_grad = False\n","\n","        self.embedding_finetune = embedding_finetune\n","        \n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    def load_embedding(self, embedding):\n","        \"\"\"\n","        :param embedding: pretraiend embedding, like GloVe or word2vec. Tensor with shape (vocab_size, embed_dim)\n","        \"\"\"\n","        self.embedding.from_pretrained(embedding, freeze=not self.embedding_finetune)\n","\n","    def init_hidden_states(self, encoder_output):\n","        \"\"\"\n","        method to initialize hidden states. must be run at beginning of any forward propagation\n","        :param encoder_output: encoded output directly from encoder. shape (batch_size, L, encoder_dim=D)\n","        :return: initialized h and c, using self.init_h and self.init_c.\n","        \"\"\"\n","        mean_encoder_output = encoder_output.mean(dim=1)  # mean over L\n","        init_h = self.init_h(mean_encoder_output)\n","        init_c = self.init_c(mean_encoder_output)\n","\n","        return init_h, init_c\n","\n","    def deep_output_layer(self, embedded_caption, h, context_vector):\n","        \"\"\"\n","        :param embedded_caption: embedded caption, a tensor with shape (batch_size, embed_dim)\n","        :param h: hidden state, a tensor with shape (batch_size, decoder_dim\n","        :param context_vector: context vector, a tensor with shape (batch_size, encoder_dim)\n","        :return: output\n","        \"\"\"\n","        scores = self.L_o(self.dropout(embedded_caption + self.L_h(h) + self.L_z(context_vector)))\n","        return scores\n","\n","    def forward(self, encoder_output, captions):\n","        \"\"\"\n","        forward method to be used at training time, because it requires captions as input\n","        :param encoder_output: encoder output, a tensor with shape (batch_size, L, encoded_dim=D)\n","        :param captions: captions encoded, a tensor with shape (batch_size, max_caption_length)\n","                         ex. [<start>, w1, w2, ... , wn, <end>]\n","        :param lengths: list, true length of each caption\n","        :return: predictions, alphas maybe?\n","        \"\"\"\n","\n","        batch_size = encoder_output.shape[0]\n","        num_pixels = encoder_output.shape[1]\n","        max_caption_length = captions.shape[-1]\n","\n","        predictions = torch.zeros(batch_size, max_caption_length - 1, self.vocab_size).to(self.device)\n","        alphas = torch.zeros(batch_size, max_caption_length - 1, num_pixels)  # save attention\n","\n","        embedded_captions = self.embedding(captions)  # (batch_size, max_caption_length, embed_dim)\n","\n","        h, c = self.init_hidden_states(encoder_output)\n","\n","        for t in range(max_caption_length - 1):  # don't need prediction when y_t-1 is <end>\n","            embedded_caption_t = embedded_captions[:, t, :]  # (batch_size, embed_dim)\n","            context_vector, alpha = self.attention(encoder_output, h)\n","            # context vector has size (batch_size, encoder_dim)\n","            h, c = self.lstm(torch.cat([embedded_caption_t, context_vector], dim=1),  # lstm input has shape (batch_size, embed_dim + encoder_dim)\n","                             (h, c))\n","            preds = self.deep_output_layer(embedded_caption_t, h, context_vector)\n","            predictions[:, t, :] = preds\n","            alphas[:, t, :] = alpha\n","\n","        return predictions, alphas\n","\n","    def generate_caption_greedily(self, encoder_output, start_token, end_token):\n","        \"\"\"\n","        greedily generate captions for encoded images.\n","        :param encoder_output: encoder output, a tensor with shape (batch_size, L, encoded_dim)\n","        :return: captions generated greedily\n","        \"\"\"\n","        # TODO\n","        self.eval()\n","        \n","        h, c = self.init_hidden_states(encoder_output)\n","        captions = [start_token]\n","        alphas = []\n","        with torch.no_grad():\n","            while captions[-1] != end_token and len(captions) < 30:  # 1 is '.'\n","                caption = captions[-1]\n","                embedded_caption = self.embedding(torch.LongTensor([caption]).to(self.device))  # (1, embed_dim)\n","                context_vector, alpha = self.attention(encoder_output, h)  # (1, encoder_dim)\n","                h, c = self.lstm(torch.cat([embedded_caption, context_vector], dim=1),\n","                                (h, c))\n","                preds = self.deep_output_layer(embedded_caption, h, context_vector)  # (1, vocab_size)\n","                next_word = int(torch.argmax(preds, dim=1, keepdim=True).squeeze())\n","                captions.append(next_word)\n","                alphas.append(alpha)\n","\n","        return captions, alphas\n","            "],"execution_count":null,"outputs":[]}]}