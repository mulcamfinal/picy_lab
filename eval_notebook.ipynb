{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"eval_notebook.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1G-ro_Y41ZBpRJMotRkC4Fg_as8zcs_AH","authorship_tag":"ABX9TyOYmcVa3LLKRQPyO6rsPVI1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Pp-TyUjTnIVR"},"source":["!pip install konlpy\n","!pip install tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zr08h4LlRNDw","executionInfo":{"status":"ok","timestamp":1635168589495,"user_tz":-540,"elapsed":308,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}}},"source":["import os\n","import json\n","import re\n","\n","from tqdm import tqdm\n","from konlpy.tag import Okt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","from PIL import Image\n","from PIL import ImageFile"],"execution_count":97,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxVCaG_TcC_U","executionInfo":{"status":"ok","timestamp":1635168592114,"user_tz":-540,"elapsed":6,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}},"outputId":"9a37b88b-f014-4500-bcc9-9600baae056a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"vydEFnihpUzL"},"source":["# 이미지 파일 복사후 이름변경, 경로에 이동\n","\n","# 이미지가 들어오면 input 에 맞는 json 파일 생성\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"foPjq6sdIoaH","executionInfo":{"status":"ok","timestamp":1635168595736,"user_tz":-540,"elapsed":366,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}},"outputId":"6e2b3273-6cdd-45f9-ec01-9f28f53991b0"},"source":["with open('/content/drive/MyDrive/gh/picy_lab/test.json', encoding='utf8') as json_file:\n","    test = json.load(json_file)\n","test"],"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'hashtag': ['#대구타워',\n","   '#83타워',\n","   '#83애슐리',\n","   '#불빛축제',\n","   '#결혼기념일',\n","   '#매년가는중',\n","   '#수다남매'],\n","  'image_path': 'tower_999.jpg',\n","  'img_name': '999',\n","  'label': 'tower',\n","  'likes': '17',\n","  'text': '83타워 바로~~ 아래에서 올려다보고 찍었다'},\n"," {'hashtag': ['#서울', '#63빌딩', '#한강', '#미세먼지', '#겨울'],\n","  'image_path': 'tower_1000.jpg',\n","  'img_name': '1000',\n","  'label': 'tower',\n","  'likes': '16',\n","  'text': '군자 가는 길에 63빌딩 한컷!..'}]"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","metadata":{"id":"B8VOvsCLMTLu","executionInfo":{"status":"ok","timestamp":1635168599853,"user_tz":-540,"elapsed":429,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}}},"source":["# config\n","\n","okt = Okt()\n","def tokenize_fn(text):\n","    tokens = okt.pos(text, norm=True, join=True)\n","    return tokens\n","\n","vocab_pass = '/content/drive/MyDrive/gh/picy_lab/vocabs/text_vocab_1.json'\n","cp_pass = '/content/drive/MyDrive/gh/picy_lab/checkpoint/resnext_lb_text_1.pth'\n","\n","JSON_FILES = {\n","    'test': '/content/drive/MyDrive/gh/picy_lab/test.json',\n","}\n","\n","class Args():\n","    def __init__(self):\n","        self.model = 'resnext_lb'\n","        self.mode = 'test'\n","        self.target_type = 'text'\n","        self.vocab_file = vocab_pass\n","        self.checkpoint_load_path = cp_pass\n","        self.load_image_on_ram = True\n","\n","class Config:\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # encoder\n","    encoded_size = 14  # size of encoded image\n","    encoder_finetune = False\n","    # decoder\n","    encoder_dim = 1024\n","    decoder_dim = 512\n","    attention_dim = 512\n","    dropout = 0.5\n","    embed_dim = 400\n","    embedding_finetune = True\n","\n","    lr = 5e-4\n","\n","    # dataloader\n","    batch_size = 30\n","    num_workers = 4\n","    max_text_length = 30\n","    max_hashtag_length = 25\n","\n","    # train\n","    num_epochs = 20\n","    log_every = 30\n","    validation_freq = 1\n","    patience = 5\n","    \n","    checkpoint_path = \"checkpont/model.pth\"\n","\n","args=Args()"],"execution_count":100,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOJO-plODy0b","executionInfo":{"status":"ok","timestamp":1635168601812,"user_tz":-540,"elapsed":359,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}}},"source":["## dataloader\n","\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","#from config import Config\n","\n","\n","target_size = 224\n","default_transform = transforms.Compose([\n","            transforms.Resize(target_size),\n","            transforms.CenterCrop(target_size),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225]),\n","        ])\n","\n","\n","class PICYDataset(Dataset):\n","    def __init__(self, json_file, vocab, transform=default_transform, type='hashtag', tokenize_fn=None,\n","                 load_on_ram=False):\n","        \"\"\"\n","        json_file: path to json file\n","        vocab: dictionary object. must include <PAD>, <UNK>, <start>, <end> as its keys.\n","        transform: transform to be applied to image\n","        type: 'hashtag' or 'text'\n","        tokenize_fn: function that is used to tokenize text when type is 'text'. input: string, output: list of strings.\n","        \"\"\"\n","        \n","        assert any(json_file.endswith(file_type) for file_type in ['total.json', 'train.json', 'val.json', 'test.json'])\n","        assert type in ['hashtag', 'text']\n","        assert '<UNK>' in vocab\n","        assert '<start>' in vocab\n","        assert '<end>' in vocab\n","        \n","        if type == 'text':\n","            assert tokenize_fn is not None\n","        self.vocab = vocab\n","            \n","        self.root_dir = '/'.join(json_file.split('/')[:-1])\n","        self.type = type\n","        self.vocab = vocab\n","        \n","        self.transform = transform\n","        self.tokenize_fn = tokenize_fn\n","        self.load_on_ram = load_on_ram\n","        \n","        with open(json_file,encoding='utf8') as fr:\n","            # remove empty data\n","            d = json.load(fr)\n","            self.json = []\n","            for item in d:\n","                if type not in item:\n","                    continue\n","                if type == 'text':\n","                    # vocab에 포함되는 단어가 없으면 데이터셋에 포함하지 않음\n","                    target = [token for token in tokenize_fn(item['text']) if token in self.vocab]\n","                else:\n","                    target = [t for t in item['hashtag'] if t in self.vocab]\n","                if target != []:\n","                    # 길이가 너무 큰 타겟 제외...\n","                    if type == 'text':\n","                        if len(target) > Config.max_text_length:\n","                            continue\n","                    elif type == 'hashtag':\n","                        if len(target) > Config.max_hashtag_length:\n","                            continue\n","                    self.json.append(item)\n","                    \n","        # target 미리 생성\n","        self.targets = []\n","        self.tokens = []\n","        for item in self.json:\n","            if type == 'text':\n","                text = item['text']\n","                tokens = self.tokenize_fn(text)\n","                self.tokens.append(tokens)\n","                UNK_idx = self.vocab['<UNK>']\n","                target = [self.vocab.get(token, UNK_idx) for token in tokens]\n","            elif type == 'hashtag':\n","                # 해시태그의 경우, vocab에 존재하지 않으면 그냥 무시\n","                hashtags = item['hashtag']\n","                self.tokens.append(hashtags)\n","                target = [self.vocab.get(hashtag) for hashtag in hashtags if hashtag in self.vocab]\n","            self.targets.append(target)\n","        \n","        # 이미지를 미리 불러온다\n","        if self.load_on_ram:\n","            self.images = []\n","            for item in self.json:\n","                image = Image.open(os.path.join(self.root_dir, item['image_path']))\n","                if self.transform is not None:\n","                    image = self.transform(image)\n","                self.images.append(image)\n","                    \n","    def __getitem__(self, index):\n","        item = self.json[index]\n","        \n","        # load target = hashtag or text\n","        target = self.targets[index]\n","        target = [self.vocab.get('<start>')] + target + [self.vocab.get('<end>')]\n","        target = torch.LongTensor(target)\n","        \n","        # load image\n","        if self.load_on_ram:\n","            image = self.images[index]\n","        else:\n","            image = Image.open(os.path.join(self.root_dir, item['image_path']))\n","            if self.transform is not None:\n","                image = self.transform(image)\n","            \n","        return image, target\n","    \n","    def __len__(self):\n","        return len(self.json)\n","    \n","    \n","def collate_fn(data):\n","    \"\"\"\n","    data: list of tuple (image, target)\n","    \n","    returns:\n","        images: torch tensor of shape (batch_size, 3, img_height, img_width)\n","        targets: torch tensor of shape (batch_size, padded_length)\n","        lengths: list of 'real' length of each target\n","    \"\"\"\n","    \n","    # Sort a data list by caption length (descending order).\n","    data.sort(key=lambda x: len(x[1]), reverse=True)\n","    images, targets = zip(*data)\n","\n","    # Merge images (from tuple of 3D tensor to 4D tensor).\n","    images = torch.stack(images, 0)\n","\n","    # Merge targets (from tuple of 1D tensor to 2D tensor).\n","    lengths = [len(cap) for cap in targets]\n","    padded_targets = torch.zeros(len(targets), max(lengths)).long()\n","    for i, cap in enumerate(targets):\n","        end = lengths[i]\n","        padded_targets[i, :end] = cap[:end]        \n","    return images, padded_targets, lengths\n","\n","\n","def get_dataloader(json_file, vocab, transform=default_transform, type='hashtag', tokenize_fn=None,\n","                   batch_size=32, shuffle=True, num_workers=-1, load_on_ram=False):\n","    \n","    dataset = PICYDataset(json_file, vocab, transform=transform, type=type, tokenize_fn=tokenize_fn, load_on_ram=load_on_ram)\n","    \n","    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n","    \n","    return loader\n","        "],"execution_count":101,"outputs":[]},{"cell_type":"code","metadata":{"id":"1YoLUhRFBD6p","executionInfo":{"status":"ok","timestamp":1635168603346,"user_tz":-540,"elapsed":355,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}}},"source":["# model\n","\n","class ResNextEncoder(nn.Module):\n","    \"\"\"\n","    An encoder that encodes each input image to tensor with shape (L, D)\n","    resnet101 is used for pretrained convolutional network.\n","    \"\"\"\n","    def __init__(self, encoded_size=14, encoder_finetune=False):\n","        \"\"\"\n","\n","        :param encoded_size: size of image after being encoded.\n","        :param allow_finetune: if allow finetune, then encoder conv network is also trained.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.encoded_size = encoded_size\n","\n","        #resnext_wsl = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\n","        #resnext_wsl = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')\n","        torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n","        resnext_wsl = torch.hub.load('pytorch/vision:v0.8.0', 'resnext101_32x8d', pretrained=True)\n","        \n","        \n","        layers_to_use = list(resnext_wsl.children())[:-3]\n","\n","        self.conv_net = nn.Sequential(*layers_to_use)\n","\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_size, encoded_size))\n","\n","        self.encoder_finetune = encoder_finetune\n","        if not encoder_finetune:\n","            self.conv_net.eval()\n","\n","    def forward(self, images):\n","        \"\"\"\n","\n","        :param images: Tensor with shape (batch_size, 3, image_size, image_size)\n","        :return:\n","        \"\"\"\n","        if not self.encoder_finetune:\n","            with torch.no_grad():\n","                x = self.conv_net(images)\n","        else:\n","            x = self.conv_net(images)  # (batch_size, encoder_dim, image_size/32, image_size/32\n","        x = self.adaptive_pool(x)  # (batch_size, encoder_dim, self.encoded_size, self.encoded_size\n","        x = x.permute(0, 2, 3, 1)  # (batch_size, self.encoded_size, self.encoded_size, encoder_dim\n","        # since shape (batch_size, self.encoded_size ** 2, encoder_dim) will be used in decoder, do permutation\n","\n","        batch_size = x.shape[0]\n","        encoder_dim = x.shape[-1]\n","        x = x.view(batch_size, -1, encoder_dim)  # (batch_size, L, D)\n","        # each point l in encoded image has vector with D-dim that represents that point\n","        # self.encoded_size ** 2 will be L and encoder_dim will be D in original paper's notation\n","        return x\n","\n","\n","class LookBackAttention(nn.Module):\n","    \"\"\"\n","    Deterministic \"soft\" attention, which is differentiable and thus can be learned by backpropagation\n","    \"\"\"\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        \"\"\"\n","        :param encoder_dim: feature size of encoded image = last dimension of encoder output.\n","        :param decoder_dim: dimension of decoder's hidden state\n","        :param attention_dim: size of attention network. does not affect output dimension\n","        \"\"\"\n","        super().__init__()\n","        self.encoder_attention = nn.Linear(encoder_dim, attention_dim)\n","        self.decoder_context_vector_attention = nn.Linear(decoder_dim + encoder_dim, attention_dim)\n","        self.attention = nn.Linear(attention_dim, 1)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, encoder_output, decoder_hidden, context_vector):\n","        encoder_att = self.encoder_attention(encoder_output)  # (batch_size, L, attention_dim)\n","        decoder_context_vector_att = self.decoder_context_vector_attention(torch.cat([decoder_hidden, context_vector], dim=1))  # (batch_size, attention_dim)\n","        encoder_plus_decoder_att = encoder_att + decoder_context_vector_att.unsqueeze(1)  # (batch_size, L, attention_dim)\n","        attention = self.attention(F.relu(encoder_plus_decoder_att)).squeeze(2)  # (batch_size, L)\n","        alpha = self.softmax(attention)  # (batch_size, L)\n","        context_vector = (encoder_output * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n","        # sum(dim=1) means summing over L\n","        # context vector is z_hat in original paper, calculated from h_t-1, and encoder output a\n","\n","        return context_vector, alpha  # keep alpha for visualization?\n","\n","\n","class LookBackDecoder(nn.Module):\n","    \"\"\"\n","    Decoder with attention\n","    \"\"\"\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim, embed_dim, vocab_size,\n","                 dropout=0.5, embedding_finetune=True):\n","        \"\"\"\n","        :param encoder_dim: feature size of encoded image = last dimension of encoder output.\n","        :param decoder_dim: dimension of decoder's hidden state\n","        :param attention_dim: size of attention network. does not affect output dimension\n","        :param embed_dim: dimension of word embedding\n","        :param vocab_size: size of vocabulary\n","        :param dropout: dropout rate\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        self.encoder_dim = encoder_dim\n","        self.decoder_dim = decoder_dim\n","        self.attention_dim = attention_dim\n","        self.embed_dim = embed_dim\n","        self.vocab_size = vocab_size\n","        self.dropout_rate = dropout\n","\n","        self.look_back_attention = LookBackAttention(encoder_dim, decoder_dim, attention_dim)\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","\n","        self.lstm = nn.LSTMCell(self.encoder_dim + self.embed_dim, self.decoder_dim, bias=True)\n","        # h and c are initialized from encoder output.\n","        # authors used MLP, for now, use single layer perceptron\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim, bias=False)\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim, bias=False)\n","\n","        # deep output layers\n","        self.L_h = nn.Linear(decoder_dim, embed_dim, bias=False)\n","        self.L_z = nn.Linear(encoder_dim, embed_dim, bias=False)\n","        self.L_o = nn.Linear(embed_dim, vocab_size, bias=False)\n","\n","        self.dropout = nn.Dropout(p=self.dropout_rate)\n","\n","        if not embedding_finetune:\n","            # always set embedding_finetune == True when not using pretrained embeddings\n","            for param in self.embedding.parameters():\n","                param.requires_grad = False\n","\n","        self.embedding_finetune = embedding_finetune\n","        \n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    def load_embedding(self, embedding):\n","        \"\"\"\n","        :param embedding: pretraiend embedding, like GloVe or word2vec. Tensor with shape (vocab_size, embed_dim)\n","        \"\"\"\n","        self.embedding.from_pretrained(embedding, freeze=not self.embedding_finetune)\n","\n","    def init_hidden_states(self, encoder_output):\n","        \"\"\"\n","        method to initialize hidden states. must be run at beginning of any forward propagation\n","        :param encoder_output: encoded output directly from encoder. shape (batch_size, L, encoder_dim=D)\n","        :return: initialized h and c, using self.init_h and self.init_c.\n","        \"\"\"\n","        mean_encoder_output = encoder_output.mean(dim=1)  # mean over L\n","        init_h = self.init_h(mean_encoder_output)\n","        init_c = self.init_c(mean_encoder_output)\n","\n","        return init_h, init_c\n","\n","    def deep_output_layer(self, embedded_caption, h, context_vector):\n","        \"\"\"\n","        :param embedded_caption: embedded caption, a tensor with shape (batch_size, embed_dim)\n","        :param h: hidden state, a tensor with shape (batch_size, decoder_dim\n","        :param context_vector: context vector, a tensor with shape (batch_size, encoder_dim)\n","        :return: output\n","        \"\"\"\n","\n","        scores = self.L_o(self.dropout(embedded_caption + self.L_h(h) + self.L_z(context_vector)))\n","        return scores\n","\n","    def forward(self, encoder_output, captions):\n","        \"\"\"\n","        forward method to be used at training time, because it requires captions as input\n","\n","        :param encoder_output: encoder output, a tensor with shape (batch_size, L, encoded_dim=D)\n","        :param captions: captions encoded, a tensor with shape (batch_size, max_caption_length)\n","                         ex. [<start>, w1, w2, ... , wn, <end>]\n","        :param lengths: list, true length of each caption\n","        :return: predictions, alphas maybe?\n","        \"\"\"\n","\n","        batch_size = encoder_output.shape[0]\n","        num_pixels = encoder_output.shape[1]\n","        max_caption_length = captions.shape[-1]\n","\n","        predictions = torch.zeros(batch_size, max_caption_length - 1, self.vocab_size).to(self.device)\n","        alphas = torch.zeros(batch_size, max_caption_length - 1, num_pixels)  # save attention\n","\n","        embedded_captions = self.embedding(captions)  # (batch_size, max_caption_length, embed_dim)\n","\n","        h, c = self.init_hidden_states(encoder_output)\n","\n","        # initial context vector\n","        context_vector = encoder_output.mean(dim=1)\n","        for t in range(max_caption_length - 1):  # don't need prediction when y_t-1 is <end>\n","            embedded_caption_t = embedded_captions[:, t, :]  # (batch_size, embed_dim)\n","            context_vector, alpha = self.look_back_attention(encoder_output, h, context_vector)\n","            # context vector has size (batch_size, encoder_dim)\n","            h, c = self.lstm(torch.cat([embedded_caption_t, context_vector], dim=1),  # lstm input has shape (batch_size, embed_dim + encoder_dim)\n","                             (h, c))\n","            preds = self.deep_output_layer(embedded_caption_t, h, context_vector)\n","            predictions[:, t, :] = preds\n","            alphas[:, t, :] = alpha\n","\n","        return predictions, alphas\n","\n","    def generate_caption_greedily(self, encoder_output, start_token, end_token):\n","        \"\"\"\n","        greedily generate captions for encoded images.\n","\n","        :param encoder_output: encoder output, a tensor with shape (batch_size, L, encoded_dim)\n","        :return: captions generated greedily\n","        \"\"\"\n","        # TODO\n","        self.eval()\n","        h, c = self.init_hidden_states(encoder_output)\n","        captions = [start_token]\n","        alphas = []\n","        \n","        # initial context vector\n","        context_vector = encoder_output.mean(dim=1)\n","        with torch.no_grad():\n","            while captions[-1] != end_token and len(captions) < 30:  # 1 is '.'\n","                caption = captions[-1]\n","                embedded_caption = self.embedding(torch.LongTensor([caption]).to(self.device))  # (1, embed_dim)\n","                context_vector, alpha = self.look_back_attention(encoder_output, h, context_vector)  # (1, encoder_dim)\n","                h, c = self.lstm(torch.cat([embedded_caption, context_vector], dim=1),\n","                                (h, c))\n","                preds = self.deep_output_layer(embedded_caption, h, context_vector)  # (1, vocab_size)\n","                next_word = int(torch.argmax(preds, dim=1, keepdim=True).squeeze())\n","                captions.append(next_word)\n","                alphas.append(alpha)\n","        \n","        self.train()\n","\n","        return captions, alphas"],"execution_count":102,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcIn2UeyMS2I"},"source":["def test_text(encoder, decoder, test_dataloader, vocab):\n","    # text의 각 metric의 결과를 리턴\n","    actuals = test_dataloader.dataset.tokens\n","    preds = []\n","    idx2word = dict([(v, k) for k, v in vocab.items()])\n","    \n","    encoder.eval()\n","    decoder.eval()\n","    with torch.no_grad():\n","        for ix, (image, target, length) in tqdm(enumerate(test_dataloader)):\n","            image = image.to(Config.device)\n","            target = target.to(Config.device)\n","            \n","            encoded_image = encoder(image)\n","            prediction, alphas = decoder.generate_caption_greedily(encoded_image, \n","                                                                   test_dataloader.dataset.vocab['<start>'],\n","                                                                   test_dataloader.dataset.vocab['<end>'])\n","            \n","            target = target[0, 1:-1]\n","            target = target.tolist()\n","            \n","            prediction = prediction[1:-1]\n","            prediction = [idx2word[idx] for idx in prediction]\n","            \n","            actuals.append(target)\n","            preds.append(prediction)\n","\n","            ### 조건에 따라 이어붙이기.\n","            ### 함수화 후 util로 옮기기 \n","            print()\n","            my_list = []\n","            for i in preds[-1]:\n","                my_list.extend(re.compile('[가-힣]+').findall(i))\n","            s = \"\".join(my_list)\n","            print(s)\n","            ###\n","    \n","    return 0,0,0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"04FwY8Ikp3Sr"},"source":["# Do test\n","assert args.vocab_file is not None\n","assert args.checkpoint_load_path is not None\n","\n","print(\"Loading vocab...\")\n","with open(args.vocab_file) as fr:\n","    vocab = json.load(fr)\n","    \n","print(\"Loading model...\")\n","print(args.model)\n","if args.model == 'showatt':\n","    encoder = Encoder(Config.encoded_size)\n","    decoder = Decoder(Config.encoder_dim, Config.decoder_dim, Config.attention_dim, Config.embed_dim, len(vocab))\n","elif args.model == 'resnext_lb':\n","    encoder = ResNextEncoder(Config.encoded_size)\n","    decoder = LookBackDecoder(Config.encoder_dim, Config.decoder_dim, Config.attention_dim, Config.embed_dim, len(vocab))\n","elif args.model == 'resnext':\n","    encoder = ResNextEncoder(Config.encoded_size)\n","    decoder = Decoder(Config.encoder_dim, Config.decoder_dim, Config.attention_dim, Config.embed_dim, len(vocab))\n","else:\n","    # lookback\n","    encoder = Encoder(Config.encoded_size)\n","    decoder = LookBackDecoder(Config.encoder_dim, Config.decoder_dim, Config.attention_dim, Config.embed_dim, len(vocab))\n","\n","encoder = encoder.to(Config.device)\n","decoder = decoder.to(Config.device)\n","\n","def load_model(encoder, decoder, checkpoint_path):\n","    checkpoint = torch.load(checkpoint_path, map_location=Config.device)\n","    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n","    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n","\n","load_model(encoder, decoder, args.checkpoint_load_path)\n","\n","encoder.eval()\n","decoder.eval()\n","\n","test_dataloader = get_dataloader(JSON_FILES['test'], vocab, type=args.target_type, tokenize_fn=tokenize_fn,\n","                                batch_size=1, num_workers=Config.num_workers, load_on_ram=args.load_image_on_ram,\n","                                shuffle=False)\n","\n","print(\"Running test...\")\n","if args.target_type == 'hashtag':\n","    f1, prec, rec = test_hashtag(encoder, decoder, test_dataloader, vocab)\n","    #\n","elif args.target_type == 'text':\n","    bleu1, rouge_l, meteor = test_text(encoder, decoder, test_dataloader, vocab)\n","    #"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7FC7jcrrKs9o"},"source":[""],"execution_count":null,"outputs":[]}]}